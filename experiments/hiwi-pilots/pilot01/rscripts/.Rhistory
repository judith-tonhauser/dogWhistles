geom_point() +
geom_errorbar(aes(ymin=YMin, ymax=YMax))+
#geom_text(aes(label=participantID), vjust = 1, cex= 5)+
ylab("Mean response to good controls")
# exclude all outliers identified above
d <- d %>%
filter(!(participantID %in% outliers$participantID)) %>%
droplevels()
length(unique(d$participantID)) #370, so 23 participants excluded
# exclude turkers who always clicked on roughly the same point on the scale
# on the target trials
table(d$task)
variances = d %>%
filter(task == "target") %>%
group_by(participantID) %>%
summarize(Variance = var(response)) %>%
mutate(TooSmall = Variance < mean(Variance) - 2*sd(Variance))
variances
lowvarworkers = as.character(variances[variances$TooSmall,]$participantID)
summary(variances)
lowvarworkers # 0 participants consistently clicked on roughly the same point on the scale
# adjust the names of the predicates
d = d %>%
mutate(expression = recode(expression, "right" = "be right", "inform_Sam" = "inform", "annoyed" = "be annoyed"))
# age and gender of remaining participants
table(d$age) #19-80
length(which(is.na(d$age))) # 0 missing values
# exclude outliers (0, 3330) before calculating mean
mean(d[10 < d$age & d$age < 100,]$age,na.rm=TRUE) #40.7
d %>%
select(gender, participantID) %>%
unique() %>%
group_by(gender) %>%
summarize(count=n())
write_tsv(d, file="../data/cd.tsv")
# how many data points?
table(d$expression)
table(d$cc)
# how many data points per predicate/CC/context combination?
d %>%
select(expression, cc, context) %>%
unique() %>%
group_by(response) %>%
summarize(count=n())
# how many data points per predicate/CC/context combination?
names(d)
d %>%
select(expression, cc, context, response) %>%
unique() %>%
group_by(response) %>%
summarize(count=n())
d %>%
select(expression, cc, context) %>%
unique() %>%
group_by(cc) %>%
summarize(count=n())
d %>%
select(expression, cc, context) %>%
unique() %>%
group_by(expression, cc, context) %>%
summarize(count=n())
d %>%
select(expression, cc, context) %>%
unique() %>%
group_by(expression, cc, context) %>%
summarize(count=n())
print(n=1000, count)
input_names = names(df)[c(expression, cc, context)]
input_names = names(d)[c(expression, cc, context)]
# how many data points per predicate/CC/context combination?
names(d)
input_names = c(expression, cc, context)
d %>% count_(input_names) %>% unite_("ComboVar",input_names,sep="")
d %>% count(input_names) %>% unite_("ComboVar",input_names,sep="")
d %>% count(input_names) %>% unite("ComboVar",input_names,sep="")
input_names = c("expression", "cc", "context")
d %>% count(input_names) %>% unite("ComboVar",input_names,sep="")
d %>% count(input_names) %>% unite("ComboVar",("expression", "cc", "context"),sep="")
d %>% count(input_names) %>% unite("ComboVar",c("expression", "cc", "context"),sep="")
d %>% count(c("expression", "cc", "context")) %>% unite("ComboVar",c("expression", "cc", "context"),sep="")
rlang::last_error()
help(paste)
d %>%
paste(c(expression, cc, context), sep="-")
d %>%
paste(c(expression,cc,context), sep="-")
d %>%
group_by(expression,cc,context) %>% tally
d %>%
group_by(expression,cc,context) %>% tally
tmp = d %>%
group_by(expression,cc,context) %>%
tally
tmp
min(tmp)
min(tmp$n)
mean(tmp$n)
max(tmp$n)
view(tmp)
tmp = d %>%
filter(cc != "noCC") %>%
group_by(expression,cc,context) %>%
tally
tmp
min(tmp$n) #1
mean(tmp$n) #10.6
max(tmp$n)
view(tmp)
nrow(tmp)
# how many data points per predicate/context combination?
tmp = d %>%
filter(cc != "noCC") %>%
group_by(expression,context) %>%
tally
tmp
nrow(tmp) # 1185 (if everything had been chosen, should be 1200 = 3 contexts x 400 predicate/cc combinations)
min(tmp$n) #1
min(tmp$n) #59
mean(tmp$n) #6.2
max(tmp$n) #23
tmp
median(tmp$n)
sd(tmp$n)
# how many data points for each predicate in the explicit ignorance context?
tmp = d %>%
filter(context == "explicitIgnorance") %>%
filter(cc != "noCC") %>%
group_by(expression,context) %>%
tally
tmp
nrow(tmp) # 60 combinations (3 contexts x 20 predicates)
sd(tmp$n)
min(tmp$n) #59
mean(tmp$n) #123
median(tmp$n) #80
max(tmp$n) #234
# how many data points for each predicate in the two neutral contexts?
tmp = d %>%
filter(context != "explicitIgnorance") %>%
filter(cc != "noCC") %>%
group_by(expression,context) %>%
tally
tmp
nrow(tmp) # 20 combinations (1 contexts x 20 predicates)
sd(tmp$n) #8.7
min(tmp$n) #200
mean(tmp$n) #222
median(tmp$n) #224.5
max(tmp$n) #234
# get data from Degen & Tonhauser, 2022 (Language)
# https://github.com/judith-tonhauser/projective-probability/tree/master/results/5-projectivity-no-fact
dt <- read_csv("https://raw.githubusercontent.com/judith-tonhauser/projective-probability/master/results/5-projectivity-no-fact/data/cd.csv")
summary(dt)
table(dt$verb)
# target data
dt <- dt %>%
filter(verb != "MC")
table(dt$verb)
dt.means = dt %>%
group_by(verb) %>%
summarize(MeanCertain = mean(response), CILow = ci.low(response), CIHigh = ci.high(response)) %>%
mutate(YMinCertain = MeanCertain - CILow, YMaxCertain = MeanCertain + CIHigh, expression = fct_reorder(as.factor(verb),MeanCertain))
dt.means
levels(dt.means$expression)
# reduce the acceptability data to the 20 predicates in the explicit ignorance context
t = d %>%
filter(context == "explicitIgnorance") %>%
filter(expression != "controlGood1" & expression != "controlGood2" & expression != "again"
& expression != "also" & expression != "too" & expression != "continue"
& expression != "stop" & expression != "cleft")
# calculate mean naturalness rating by expression
nat.means = t %>%
group_by(expression) %>%
summarize(Mean = mean(response), CILow = ci.low(response), CIHigh = ci.high(response)) %>%
mutate(YMin = Mean - CILow, YMax = Mean + CIHigh, expression = fct_reorder(as.factor(expression),Mean))
nat.means
# bind the data
data = left_join(nat.means, dt.means, by = "expression")
data
summary(data)
# plot of naturalness means against certainty means
ggplot(data, aes(x=Mean, y=MeanCertain),label = expression) +
geom_point(shape=21,stroke=.5,size=2,color="black") +
#geom_smooth(method="lm") +
geom_text_repel(aes(label = expression),
#box.padding   = 0.35,
point.padding = 0.5,
segment.color = 'grey50') +
geom_errorbarh(aes(xmin=YMin,xmax=YMax),height=.01,color="black") +
geom_errorbar(aes(ymin=YMinCertain,ymax=YMaxCertain),width=.01,color="black") +
#geom_point(data = nt[nt$context != "explicitIgnorance",], aes(x=expression, y=response), shape=21,fill="gray60", alpha=.5, color="blue") +
scale_x_continuous(limits = c(0,1),breaks = c(0,0.2,0.4,0.6,0.8,1.0)) +
scale_y_continuous(limits = c(0,1),breaks = c(0,0.2,0.4,0.6,0.8,1.0)) +
guides(fill=FALSE) +
geom_abline(intercept = 1,slope = -1, col="red", lty = "dashed") +
#theme(text = element_text(size=12), axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
#theme(legend.position="top") +
coord_fixed() +
xlab("Mean naturalness rating \n in explicit ignorance context") +
ylab("Mean certainty rating \n (from Degen & Tonhauser 2022)")
#theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1))
ggsave("../graphs/mean-acceptability-against-mean-certainty.pdf",height=5,width=5)
# calculate Spearman rank correlation
corr <- cor.test(x=data$Mean, y=data$MeanCertain, method = 'spearman', exact = FALSE)
corr
# order the predicate by Language paper certainty means
# https://github.com/judith-tonhauser/projective-probability/tree/master/results/5-projectivity-no-fact
dt <- read_csv("https://raw.githubusercontent.com/judith-tonhauser/projective-probability/master/results/5-projectivity-no-fact/data/cd.csv")
summary(dt)
table(dt$verb)
# target data
dt <- dt %>%
filter(verb != "MC")
# target data
tmp <- tmp %>%
filter(verb != "MC") %>%
group_by(verb) %>%
summarize(MeanCertain = mean(response)) %>%
mutate(expression = fct_reorder(as.factor(verb),MeanCertain))
# order the predicate by Language paper certainty means
# https://github.com/judith-tonhauser/projective-probability/tree/master/results/5-projectivity-no-fact
tmp <- read_csv("https://raw.githubusercontent.com/judith-tonhauser/projective-probability/master/results/5-projectivity-no-fact/data/cd.csv")
summary(tmp)
# target data
tmp <- tmp %>%
filter(verb != "MC") %>%
group_by(verb) %>%
summarize(MeanCertain = mean(response)) %>%
mutate(expression = fct_reorder(as.factor(verb),MeanCertain))
tmp
nat.means = d %>%
filter(expression != "practice" & expression != "controlGood1" & expression != "controlGood2" & expression != "controlGood3" & expression != "controlGood4") %>%
filter(expression != "also" & expression != "too" & expression != "again" & expression != "cleft" &
expression != "stop" & expression != "continue") %>%
mutate(context2 = recode(context, "factL" = "neutral", "factH" = "neutral")) %>%
group_by(expression,context) %>%
summarize(Mean = mean(response), CILow = ci.low(response), CIHigh = ci.high(response)) %>%
mutate(YMin = Mean - CILow, YMax = Mean + CIHigh) %>%
select(-c(CILow,CIHigh))
nat.means
# order the predicate by Language paper certainty means
# https://github.com/judith-tonhauser/projective-probability/tree/master/results/5-projectivity-no-fact
tmp <- read_csv("https://raw.githubusercontent.com/judith-tonhauser/projective-probability/master/results/5-projectivity-no-fact/data/cd.csv")
summary(tmp)
# target data
tmp <- tmp %>%
filter(verb != "MC") %>%
group_by(verb) %>%
summarize(MeanCertain = mean(response)) %>%
mutate(expression = fct_reorder(as.factor(verb),MeanCertain))
tmp
nat.means$expression = factor(nat.means$expression, levels=tmp$expression[order(tmp$expression)], ordered=TRUE)
levels(nat.means$expression)
ggplot(nat.means, aes(x=context, y=Mean, fill = context)) +
geom_bar(stat="identity", color = "black", position=position_dodge(.9)) +
geom_errorbar(aes(ymin=YMin,ymax=YMax),width=0.1,color="black", position=position_dodge(.9)) +
scale_y_continuous(limits = c(0,1),breaks = c(0,0.2,0.4,0.6,0.8,1.0)) +
scale_fill_manual(values=c('gray40',"#56B4E9",'#E69F00'), name = "Context") +
guides(fill=FALSE) +
theme(text = element_text(size=12), axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
theme(legend.position="top") +
facet_wrap(. ~ expression) +
ylab("Mean naturalness rating") +
xlab("Context") +
theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1))
ggsave("../graphs/ORDER-by-LANGUAGE-naturalness-by-context-and-predicate.pdf",height=7,width=7)
# set working directory to directory of script
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
# load relevant packages and set background color
library(tidyverse)
theme_set(theme_bw())
# set working directory to directory of script
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
# load clean data ----
d = read.csv("../data/d.csv")
nrow(d) #744
names(d)
table(d$womNachzug)
table(d$womNachzug)
# define numeric values for each position (higher = more feindlich)
d$womNachzug.num <- ifelse(d$womNachzug == "Ich stimme zu", 1,
ifelse(d$womNachzug == "Neutral", 0, -1))
d$womKopftuch.num <- ifelse(d$womKopftuch == "Ich stimme nicht zu", 1,
ifelse(d$womKopftuch == "Neutral", 0, -1))
d$womStaatsangehörigkeit.num <- ifelse(d$womStaatsangehörigkeit == "Ich stimme nicht zu", 1,
ifelse(dwomStaatsangehörigkeit == "Neutral", 0, -1))
d$womStaatsangehörigkeit.num <- ifelse(d$womStaatsangehörigkeit == "Ich stimme nicht zu", 1,
ifelse(d$womStaatsangehörigkeit == "Neutral", 0, -1))
d$womIslam.num <- ifelse(d$womIslam == "Ich stimme nicht zu", 1,
ifelse(d$womIslam == "Neutral", 0, -1))
d$womAsyl.num <- ifelse(d$womAsyl == "Ich stimme zu", 1,
ifelse(d$womAsyl == "Neutral", 0, -1))
d$mean.wom.score = (d$womNachzug.num + d$womKopftuch.num + d$womStaatsangehörigkeit.num + d$womIslam.num + d$womAsyl.num) / 5
# lowest possible: -1 (least conservative)
# highest possible: 1 (most conservative)
table(d$mean.wom.score)
d %>%
group_by(mean.wom.score) %>%
tally
d %>%
group_by(participant,mean.wom.score) %>%
tally
names(d)
d %>%
group_by(participantID,mean.wom.score) %>%
tally
# sort participants by their mean.wom.score
d = d %>%
mutate(subj = fct_reorder(as.factor(subj),mean.wom.score))
# sort participants by their mean.wom.score
d = d %>%
mutate(subj = fct_reorder(as.factor(participantID),mean.wom.score))
# sort participants by their mean.wom.score
d = d %>%
mutate(participantID = fct_reorder(as.factor(participantID),mean.wom.score))
# plot participants by their mean.wom.score, color code by party
ggplot(d, aes(x=participantID, y=mean.wom.score,color=party,fill=party)) +
geom_point(shape=21, size=3, alpha=1,color="black") +
scale_fill_manual(values=colors.party) +
xlab("Participant") +
ylab("Mean wom score (higher = more conservative)")
# plot participants by their mean.wom.score, color code by party
ggplot(d, aes(x=participantID, y=mean.wom.score,color=party,fill=party)) +
geom_point(shape=21, size=3, alpha=1,color="black") +
scale_fill_manual() +
xlab("Participant") +
ylab("Mean wom score (higher = more conservative)")
# color-blind-friendly palette
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# plot participants by their mean.wom.score, color code by party
ggplot(d, aes(x=participantID, y=mean.wom.score,color=party,fill=party)) +
geom_point(shape=21, size=3, alpha=1,color="black") +
scale_fill_manual(values=cbPalette) +
xlab("Participant") +
ylab("Mean wom score (higher = more conservative)")
ggsave("../graphs/mean-response-to-wom-questions-by-participant.pdf",height=4,width=9)
names(d)
table(d$participantID,d$gender)
d %>%
group_by(participantID,party,gender,ethnicity,mean.wom.score) %>%
tally
d %>%
group_by(participantID,party,gender,ethnicity,mean.wom.score) %>%
tally
# age
d %>%
group_by(age) %>%
tally
# gender
d %>%
group_by(gender) %>%
tally
# gender
d %>%
select(gender, participantID) %>%
unique() %>%
group_by(gender) %>%
summarize(count=n())
# age
d %>%
select(age, participantID) %>%
unique() %>%
group_by(gender) %>%
summarize(count=n())
# age
d %>%
select(age, participantID) %>%
unique() %>%
group_by(age) %>%
summarize(count=n())
# ethnicity
d %>%
select(ethnicity, participantID) %>%
unique() %>%
group_by(ethnicity) %>%
summarize(count=n())
names(d)
# education
d %>%
select(education, participantID) %>%
unique() %>%
group_by(education) %>%
summarize(count=n())
# party
d %>%
select(party, participantID) %>%
unique() %>%
group_by(party) %>%
summarize(count=n())
means = d %>%
group_by(item) %>%
summarize(Mean = mean(response)) %>%
ungroup()
names(d)
table(d$party)
# migration
d %>%
select(migration, participantID) %>%
unique() %>%
group_by(migration) %>%
summarize(count=n())
# only consider the target data
t = d %>%
filter(party != "spd")
table(t$party)
# calculate mean response for each target item
means = t %>%
group_by(item) %>%
summarize(Mean = mean(response)) %>%
ungroup()
means
ggplot(t, aes(x=item, y=response)) +
geom_boxplot()
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
geom_point(data = means, shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)") +
facet_wrap(. ~ item)
# calculate mean response for each target item
means = t %>%
group_by(item, party) %>%
summarize(Mean = mean(response)) %>%
ungroup()
means
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
geom_point(data = means, shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)") +
facet_wrap(. ~ item)
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
geom_point(data = means, aes(x=party, y=response), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)") +
facet_wrap(. ~ item)
ggplot(t, aes(x=party, y=response)) +
geom_boxplot()
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item)
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item) +
#geom_point(data = means, aes(x=party, y=response), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)")
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, rows = 3) +
#geom_point(data = means, aes(x=party, y=response), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)")
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, ncol = 3) +
#geom_point(data = means, aes(x=party, y=response), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)")
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, ncol = 2) +
#geom_point(data = means, aes(x=party, y=response), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)")
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, ncol = 5) +
#geom_point(data = means, aes(x=party, y=response), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)")
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, ncol = 5) +
geom_point(data = means, aes(x=party, y=response), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)")
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, ncol = 5) +
geom_point(data = means, aes(x=party, y=Mean), shape=20, size=3, alpha=1) +
xlab("Item") +
ylab("Mean response (higher = against migration)")
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, ncol = 5) +
geom_point(data = means, aes(x=party, y=Mean), shape=20, size=3, alpha=1, color = "blue") +
xlab("Item") +
ylab("Mean response (higher = against migration)")
against
# make a boxplot, add the mean response
ggplot(t, aes(x=party, y=response)) +
geom_boxplot() +
facet_wrap(. ~ item, ncol = 5) +
geom_point(data = means, aes(x=party, y=Mean), shape=20, size=3, alpha=1, color = "blue") +
xlab("Item") +
ylab("Mean response \n (1 = pro migration, 5 = against migration)")
ggsave("../graphs/response-by-item-and-party.pdf",height=5,width=8)
